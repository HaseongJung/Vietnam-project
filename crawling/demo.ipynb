{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests # 페이지를 요청하는 기능 \n",
    "from bs4 import BeautifulSoup # 파이썬으로 HTML을 다루는 기능\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "user_agent = \"Mozilla/5.0 (Linux; Android 9; SM-G975F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.83 Mobile Safari/537.36\"\n",
    "headers = {\n",
    "    \"User-Agent\": user_agent\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setRestaurantPages(url: str):\n",
    "    url = url   \n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # 200번대 코드가 아니면 코드가 멈춤\n",
    "    print(\"Response successful!\")\n",
    "    html = response.text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    restraurant_pages = []\n",
    "    total_restraurants = int(soup.find_all('span', class_=\"b\")[1].text)\n",
    "    main_pages = total_restraurants//30 if total_restraurants%30==0 else total_restraurants//30+1\n",
    "    for i in tqdm(range(main_pages-1), desc=\"Set restaurant pages\"):\n",
    "        if i > 0:\n",
    "            current_url = url[:48] + f'oa{i*30}' + url[47:]\n",
    "            # print(f'current_url: {current_url}')\n",
    "            response = requests.get(current_url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        for restraurant_page in soup.find_all('div', class_=\"biGQs _P fiohW alXOW NwcxK GzNcM ytVPx UTQMg RnEEZ ngXxk\"):\n",
    "            restraurant_pages.append(\"https://www.tripadvisor.com/\" + restraurant_page.find('a')['href'])\n",
    "            # print(restraurant_page.find('a')['href'])\n",
    "        time.sleep\n",
    "        # print(f'Total restraurant pages: {len(restraurant_pages)}')\n",
    "        \n",
    "    print(f'Total restraurant pages: {len(restraurant_pages)}')\n",
    "    \n",
    "    return restraurant_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset():\n",
    "    dataset = pd.DataFrame({\n",
    "        \"name\": [],\n",
    "        \"category\": [],\n",
    "        \"description\": [],\n",
    "        \"time\": [],\n",
    "        \"URL\": [],\n",
    "        \"priceLow\": [],\n",
    "        \"priceHigh\": [],\n",
    "        \"reviews\": [],\n",
    "        \"adress\": [],\n",
    "        \"rating\": []\n",
    "    })\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setWebdriver():\n",
    "    options = Options()\n",
    "    options.add_argument('user-agent=' + user_agent)\n",
    "    # options.add_argument('headless') #headless모드 브라우저가 뜨지 않고 실행됩니다.\n",
    "    # options.add_argument('--window-size= x, y') #실행되는 브라우저 크기를 지정할 수 있습니다.\n",
    "    # options.add_argument('--start-maximized') #브라우저가 최대화된 상태로 실행됩니다.\n",
    "    # options.add_argument('--start-fullscreen') #브라우저가 풀스크린 모드(F11)로 실행됩니다.\n",
    "    # options.add_argument('--blink-settings=imagesEnabled=false') #브라우저에서 이미지 로딩을 하지 않습니다.\n",
    "    options.add_argument('--mute-audio') #브라우저에 음소거 옵션을 적용합니다.\n",
    "    options.add_argument('incognito') #시크릿 모드의 브라우저가 실행됩니다.\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDataset(dataset, pages: list):\n",
    "    cnt = 0\n",
    "    for restaurant_page in tqdm(pages, desc=\"Restaurants\"):\n",
    "        response = requests.get(restaurant_page, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # name \n",
    "        name = soup.find('h1', class_=\"HjBfq\").text;    print(f'Name: {name}')\n",
    "\n",
    "        # category\n",
    "        category = ''\n",
    "        for x in soup.find('span', class_=\"DsyBj DxyfE\"):\n",
    "            if '$' not in x.text:\n",
    "                category += f'{x.text}, '\n",
    "        category = category[:-2]\n",
    "        print(f'Category: {category}')\n",
    "\n",
    "        # adress\n",
    "        adress = soup.find_all('a', class_=\"AYHFM\")[1].text;   print(f'Adress: {adress}')\n",
    "\n",
    "        # rating\n",
    "        rating = soup.find('span', class_=\"ZDEqb\").text;    print(f'Rating: {rating}')\n",
    "\n",
    "\n",
    "        # url\n",
    "        url = restaurant_page;    print(f'URL: {url}')\n",
    "\n",
    "        # price\n",
    "        try:\n",
    "            priceLow = soup.find('div', class_=\"SrqKb\").text.split(' - ')[0] \n",
    "            priceHigh = soup.find('div', class_=\"SrqKb\").text.split(' - ')[1]\n",
    "        except:\n",
    "            priceLow = np.NaN; priceHigh = np.NaN\n",
    "        print (f'PriceLow: {priceLow}, PriceHigh: {priceHigh}')\n",
    "\n",
    "        # reviews, only english reviews\n",
    "        total_reviews = int(soup.find_all('span', class_=\"count\")[0].text[1:-1].replace(',', ''));    print(f'Total_reviews: {total_reviews}')\n",
    "        review_pages = total_reviews//15 if total_reviews%15 == 0 else total_reviews//15 + 1;    print(f'Review pages: {review_pages}')\n",
    "        review_list = []\n",
    "        for i in tqdm(range(0, review_pages), desc='Review crwaling...'):\n",
    "            if i > 1:\n",
    "                url_list = url.split('-Reviews-')\n",
    "                current_url = url_list[0] + f'-Reviews-or{15*i}-' + url_list[1]\n",
    "                # print(f'current_url: {current_url}')\n",
    "                response = requests.get(current_url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "                html = response.text\n",
    "                soup = BeautifulSoup(html, \"html.parser\") \n",
    "\n",
    "            reviews = soup.find_all('p', class_=\"partial_entry\");\n",
    "            for review in reviews:\n",
    "                review_list.append(review.text)\n",
    "            #     print(review.text)\n",
    "            # print(f'Reviews: {len(review_list)}')\n",
    "\n",
    "            time.sleep(rd.uniform(0.1, 0.5))\n",
    "        print(f'Reviews: {len(review_list)}')\n",
    "\n",
    "\n",
    "        # Using selenium\n",
    "        driver.get(url)\n",
    "\n",
    "        # # time\n",
    "        # openingTime = soup.find('span', class_=\"mMkhr\").text[11:19];    \n",
    "        # closingTime = soup.find('span', class_=\"mMkhr\").text[22:30];   \n",
    "        # print(f'OpeningTime: {openingTime}, ClosingTime: {closingTime}')\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#component_50 > div > div:nth-child(3) > span.DsyBj.YTODE > div > span.mMkhr\")))\n",
    "            element = driver.find_element(By.CSS_SELECTOR, \"#component_50 > div > div:nth-child(3) > span.DsyBj.YTODE > div > span.mMkhr\")\n",
    "            time.sleep(1)\n",
    "            element.click() \n",
    "        except:\n",
    "            times = np.NaN\n",
    "\n",
    "        time_list = [] \n",
    "        time_elements = driver.find_elements(By.CLASS_NAME, \"RiEuX.f\")\n",
    "        for time_element in time_elements:\n",
    "            time_list.append(time_element.text.replace('\\n', ':'))\n",
    "        print(f'Times: {time_list}')\n",
    "\n",
    "        # description\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#component_52 > div.hILIJ > div > div:nth-child(2) > div > div > div.gmbZC > a\")))\n",
    "            element = driver.find_element(By.CSS_SELECTOR, \"#component_52 > div.hILIJ > div > div:nth-child(2) > div > div > div.gmbZC > a\")\n",
    "            element.click()\n",
    "            WebDriverWait(driver, 3).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"#BODY_BLOCK_JQUERY_REFLOW > div.VZmgo.D.X0.X1.Za > div > div.TocEc._Z.S2.H2._f > div > div > div.kwVln > div > div:nth-child(1) > div > div.jmnaM\")))\n",
    "            element = driver.find_element(By.CSS_SELECTOR, \"#BODY_BLOCK_JQUERY_REFLOW > div.VZmgo.D.X0.X1.Za > div > div.TocEc._Z.S2.H2._f > div > div > div.kwVln > div > div:nth-child(1) > div > div.jmnaM\")\n",
    "            description = element.text\n",
    "            print(description)\n",
    "        except:\n",
    "            description = np.NaN\n",
    "        print(f'Description: {description}\\n')\n",
    "        \n",
    "        time.sleep(rd.uniform(1, 2))\n",
    "\n",
    "        dataset.loc[cnt] = [name, category, description, time_list, url, priceLow, priceHigh, review_list, adress, rating]\n",
    "        cnt += 1\n",
    "        \n",
    "        \n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response successful!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set restaurant pages:  10%|█         | 5/49 [00:07<01:10,  1.59s/it]"
     ]
    }
   ],
   "source": [
    "url = \"https://www.tripadvisor.com/Restaurants-g298085-Da_Nang.html\"\n",
    "restraurant_pages = setRestaurantPages(url)\n",
    "\n",
    "# create dataset\n",
    "dataset = createDataset()\n",
    "\n",
    "# set webdriver\n",
    "driver = setWebdriver()\n",
    "\n",
    "# start crawling\n",
    "dataset = buildDataset(dataset, restraurant_pages[:5])\n",
    "\n",
    "# save dataset\n",
    "dataset.to_csv('test.csv')\n",
    "print(f'Dataset build complete!\\n')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Partial testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_reviews: 13\n",
      "Review pages: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Review crwaling...: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# reviews, only english reviews\n",
    "\n",
    "url = \"https://www.tripadvisor.com//Restaurant_Review-g298085-d24082246-Reviews-3_Big_Nu_ng_L_u-Da_Nang.html\"\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "total_reviews = int(soup.find_all('span', class_=\"count\")[0].text[1:-1].replace(',', ''));    print(f'Total_reviews: {total_reviews}')\n",
    "review_pages = total_reviews//15 if total_reviews%15 == 0 else total_reviews//15 + 1;    print(f'Review pages: {review_pages}')\n",
    "review_list = []\n",
    "for i in tqdm(range(0, review_pages), desc='Review crwaling...'):\n",
    "    if i > 1:\n",
    "        url_list = url.split('-Reviews-')\n",
    "        current_url = url_list[0] + f'-Reviews-or{15*i}-' + url_list[1]\n",
    "        # print(f'current_url: {current_url}')\n",
    "        response = requests.get(current_url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        html = response.text\n",
    "        soup = BeautifulSoup(html, \"html.parser\") \n",
    "\n",
    "    reviews = soup.find_all('p', class_=\"partial_entry\");\n",
    "    for review in reviews:\n",
    "        review_list.append(review.text)\n",
    "    #     print(review.text)\n",
    "    # print(f'Reviews: {len(review_list)}')\n",
    "\n",
    "    time.sleep(rd.uniform(0.1, 0.5))\n",
    "print(f'Reviews: {len(review_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# time test\n",
    "\n",
    "driver = setWebdriver()\n",
    "url = \"https://www.tripadvisor.com/Restaurant_Review-g15296807-d13810289-Reviews-Thia_G_Restaurant_Da_Nang-My_An_Da_Nang.html\"\n",
    "response = driver.get(url)\n",
    "\n",
    "try:\n",
    "    WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"#component_50 > div > div:nth-child(3) > span.DsyBj.YTODE > div > span.mMkhr\")))\n",
    "    element = driver.find_element(By.CSS_SELECTOR, \"#component_50 > div > div:nth-child(3) > span.DsyBj.YTODE > div > span.mMkhr\")\n",
    "    time.sleep(1)\n",
    "    element.click() \n",
    "except:\n",
    "    times = np.NaN\n",
    "\n",
    "time_list = [] \n",
    "time_elements = driver.find_elements(By.CLASS_NAME, \"RiEuX.f\")\n",
    "for time_element in time_elements:\n",
    "    time_list.append(time_element.text.replace('\\n', ':'))\n",
    "    print(time_element)\n",
    "print(time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
